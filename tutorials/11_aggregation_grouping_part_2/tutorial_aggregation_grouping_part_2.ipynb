{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 11 - Calculating Monthly Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to demonstrate how to use a custom `aggregation` function with `groupby`.\n",
    "\n",
    "In particular, we will calculate three monthly statistics for several ETFs during the year of 2018.  The three statistics we will calculate are:\n",
    "\n",
    "1. average daily volume\n",
    "\n",
    "2. monthly return\n",
    "\n",
    "3. monthly volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the packages that we will need for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> import numpy as np\n",
    "##> import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading-In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will be working with the `sector_etf_2018.csv` data set, which consists of end-of-day price data for the Select Sector SPDR ETFs.  Each of these funds tracks a particular subset (sector) of the SP&500 Index.  For example, XLF tracks the financial sector and has major holdings in JP Morgan, Wells Fargo, and Bank of America."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf = pd.read_csv('../data/sector_etf_2018.csv')\n",
    "##> df_etf.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** Use a `DataFrame` attribute to determine the number of rows and columns in `df_etf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring and Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the coding challenge, this data set is large (by our standards).  Whenever, I encounter a new data set that I can't look at in its entirety, I like to do a bit of exploration via the built-in `pandas` methods.\n",
    "\n",
    "We know we have a variety of ETFs in our data, but it would be useful to know how many (especially if we were expecting a certain number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(df_etf['symbol'].unique())\n",
    "##> print(df_etf['symbol'].unique().size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** What `DataFrame` attribute do we use to check the data types of the columns of `df_etf`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the answer to the coding challenge, the `date` column was read-in as a string rather than as a date.  This is a common problem, so `pandas` has a built in method named `pandas.to_datetime()` to address this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf['date'] = pd.to_datetime(df_etf['date'])\n",
    "##> df_etf.dtypes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I work with a time series of daily prices, I like to check the first and last trade dates that are represented in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(df_etf['date'].min())\n",
    "##> print(df_etf['date'].max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we know about our data-set thus far:\n",
    "\n",
    "1. 10 different ETFs are represented.\n",
    "\n",
    "1. Prices are coming from the entirety of 2018.\n",
    "\n",
    "\n",
    "Here are some things that we aren't necessarily sure of that would be worth checking a high-stakes situation:\n",
    "\n",
    "1. Is there a row/price for each symbol on each trade date?\n",
    "\n",
    "1. Is there ever more than one row/price for a given symbol on a given trade date?\n",
    "\n",
    "We won't bother with these questions for the purposes of this tutorial, but these are the types of data-integrity questions I will often try to answer when encountering a new data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding `year` and `month` Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate goal is to calculate monthly statistics for each of the ETFs in our data set.\n",
    "\n",
    "As a preliminary step, let's add a month and year column to the `df_etf`, by utilizing the `.dt` attribute that `pandas` provides for date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf['year'] = df_etf['date'].dt.year\n",
    "##> df_etf['month'] = df_etf['date'].dt.month\n",
    "##> df_etf[['date', 'year', 'month']].head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick data-integrity check:  There are 10 ETFs in our data set and there are 12 months in a year, so the number of symbol-year-month combinations should be 120.\n",
    "\n",
    "The following code counts the number of rows associated with each symbol-year-month combination and puts that data into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_num_rows = \\\n",
    "##>     df_etf.groupby(['symbol', 'year', 'month']).size().reset_index()\n",
    "##> df_num_rows.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** Confirm that there are the correct number of symbol-year-month combinations in `df_num_rows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've added the `year` and `month` columns we can proceed to calculating our monthly statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Daily Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the most straight-forward calculation: average daily volume, over each month, for all 10 of the ETFs in our data set.  \n",
    "\n",
    "This amounts to:\n",
    "\n",
    "1. grouping by `symbol`, `month`, and `year`\n",
    "\n",
    "1. applying the built-in `np.mean()` function to the `volume` column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_volume = \\\n",
    "##>     df_etf.groupby(['symbol', 'year', 'month'])['volume'].agg([np.mean]).reset_index()\n",
    "##> df_volume.rename(columns={'mean':'avg_volume'}, inplace=True)\n",
    "##> df_volume.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** Calculate the maximum *daily* volume for each symbol, *over the entire year*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's calculate monthly returns for each of the ETFs in our data set.  This amounts to:\n",
    "\n",
    "1. grouping by `symbol`, `month`, and `year`\n",
    "\n",
    "1. applying an aggregation function to the `daily_returns` column\n",
    "\n",
    "These are the same two steps that we have done in our previous aggregation examples.  However, there is one additional wrinkle that we are going to have to contend with. \n",
    "\n",
    "\n",
    "When we previously aggregated over groups, we used simple built-in aggregation funtions available through `numpy`, such as `np.max` and `np.mean`.  Calculating monthly returns from daily returns is a little more complicated.\n",
    "\n",
    "Thus, we are going to have to first create a custom function for calculating monthly returns from daily returns, and then use this custom function in `.agg()`.\n",
    "\n",
    "The following code defines our monthly returns function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> def monthly_ret(dly_ret):\n",
    "##>     return np.prod(1 + dly_ret) - 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply `monthly_ret` over all of our groups using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_ret = \\\n",
    "##>     df_etf.groupby(['symbol', 'month', 'year'])['dly_ret'].agg([monthly_ret]).reset_index()\n",
    "##> df_ret.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see from our calculation that in March of 2018, XLB had a monthly return of -4.5112%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** Which ETF had the highest single monthly return in all of 2018?  What was the month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a similar process to calculate the monthly volatility of each of the ETFs.\n",
    "\n",
    "We begin by defining a custom function that calculates the monthly volatility from daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> def monthly_vol(dly_ret):\n",
    "##>     return np.std(dly_ret) * np.sqrt(252)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our `monthly_vol` function in to perform an aggregating calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_vol = \\\n",
    "##>     df_etf.groupby(['symbol', 'month', 'year'])['dly_ret'].agg([monthly_vol]).reset_index()\n",
    "##> df_vol.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:**  What was the volatility for XLF in December 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Metrics - `inner join`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose that we want to combine our three metrics into one report - meaning that we want them organized into one `DataFrame` in an easy to read fashion.\n",
    "\n",
    "One way to do this is to use the `pandas.merge()` method that we learned in the previous tutorial to join together `df_volume` (average daily volume), `df_ret` (monthly returns), and `df_vol` (monthly volatility). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_joined_1 = \\\n",
    "##>     pd.merge(df_volume, df_ret, on=['symbol', 'year', 'month'])\n",
    "##> df_joined_2 = pd.merge(df_joined_1, df_vol, on=['symbol', 'year', 'month'])\n",
    "##> df_joined_2.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Metrics - multiple aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do this would be to supply all of our custom aggregation fuctions as arguments to the `.agg()` function in one shot.  \n",
    "\n",
    "Here is what that would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # defining aggregations\n",
    "##> agg_funcs = \\\n",
    "##>     {'volume':[np.mean], 'dly_ret':[monthly_ret, monthly_vol]}\n",
    "##> \n",
    "##> # performing all aggregations all three aggregations at once\n",
    "##> df_joined = \\\n",
    "##>     df_etf.groupby(['symbol', 'month', 'year']).agg(agg_funcs).reset_index()\n",
    "##> \n",
    "##> # looking at the data frame\n",
    "##> df_joined.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we the inputs into the `.agg()` method is a `dict` whose elements are pairs that look like: \n",
    "\n",
    "`'column_name':[list_of_aggregating_functions`]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** Modify the code above to add maximum daily volume to the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Reading\n",
    "\n",
    "*PDSH* - 3.7 - Combining Datasets: Merging and Joining\n",
    "\n",
    "*PDSH* - 3.8 - Aggregation and Grouping\n",
    "\n",
    "*Python for Data Analysis (McKinney)* - Chapter 9 (pp 251-274) Data Aggregation and Grouping Operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
