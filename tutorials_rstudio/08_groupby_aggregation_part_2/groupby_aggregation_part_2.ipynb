{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `.groupby()` and `.agg()` - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to demonstrate how to use `.groupby()` and `.agg()` with user defined functions.\n",
    "\n",
    "In service of this objective, our analysis goal will be to calculate three monthly statistics for several ETFs during the year of 2020.  The three statistics we will calculate are:\n",
    "\n",
    "1. average daily volume\n",
    "\n",
    "2. monthly return\n",
    "\n",
    "3. monthly volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> import numpy as np\n",
    "##> import pandas as pd\n",
    "##> import pandas_datareader as pdr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading-In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will be working with price data for the Select Sector SPDR ETFs.  Each of these funds tracks a particular subset (sector) of the SP&500 Index.  For example, XLF tracks the financial sector and has major holdings in JP Morgan, Wells Fargo, and Bank of America.\n",
    "\n",
    "Let grab the data from Yahoo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> pd.options.display.max_rows = 25\n",
    "##> lst_symbols = ['XLY', 'XLP', 'XLE', 'XLF', 'XLV', 'XLI', 'XLB', 'XLRE', 'XLK', 'XLU',]\n",
    "##> df_etf = pdr.get_data_yahoo(lst_symbols, start='2020-01-01', end='2020-12-31')\n",
    "##> df_etf = df_etf.round(2)\n",
    "##> df_etf.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is not as tidy as we would like.  Let's use method chaining to perform a series of data munging operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf = \\\n",
    "##>     (\n",
    "##>     df_etf\n",
    "##>         .stack(level='Symbols') #pivot the table\n",
    "##>         .reset_index() #turn date into a column \n",
    "##>         .sort_values(by=['Symbols', 'Date']) #sort\n",
    "##>         .rename(columns={'Date':'date', 'Symbols':'symbol', 'Adj Close':'adj_close','Close':'close', \n",
    "##>                          'High':'high', 'Low':'low', 'Open':'open', 'Volume':'volume'}) #renaming columns\n",
    "##>         [['date', 'symbol','open', 'high', 'low', 'close', 'volume', 'adj_close']] #reordering columns\n",
    "##>     )\n",
    "##> df_etf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** Use a `DataFrame` attribute to determine the number of rows and columns in `df_etf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring and Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the coding challenge, this data set is large (by our standards).  Whenever I encounter a new data set that I can't look at in its entirety, I like to do a bit of exploration via the built-in `pandas` methods.\n",
    "\n",
    "We know we have a variety of ETFs in our data, but it would be useful to know how many (especially if we were expecting a certain number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(df_etf['symbol'].unique())\n",
    "##> print(df_etf['symbol'].unique().size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** What `DataFrame` attribute could we use to check the data types of the columns of `df_etf`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I work with a time series of daily prices, I like to check the first and last trade dates that are represented in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(df_etf['date'].min())\n",
    "##> print(df_etf['date'].max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we know about our data set thus far:\n",
    "\n",
    "1. 10 different ETFs are represented.\n",
    "\n",
    "1. Prices are coming from the entirety of 2020.\n",
    "\n",
    "\n",
    "Here are some things that we aren't necessarily sure of that would be worth checking in a high-stakes situation:\n",
    "\n",
    "1. Is there a row/price for each symbol on each trade date?\n",
    "\n",
    "1. Is there ever more than one row/price for a given symbol on a given trade date?\n",
    "\n",
    "We won't bother answering these questions for the purposes of this tutorial, but these are the types of data-integrity questions I will often try to answer when encountering a new data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Daily Returns with `groupby()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ultimate goal is to calculate monthly returns and monthly volatilities for each ETF in `df_etf`.  These quantities are both functions of daily returns.  So, our first order of business is to calculate daily returns. \n",
    "\n",
    "In a previous tutorial we calculated daily returns in a simple vectorized fashion.  Unfortunately, we can't use the exact same approach here because there are multiple ETFs in the data set.\n",
    "\n",
    "To overcome this challenge we will use our first application of `.groupby()`.\n",
    "\n",
    "Here is the `.groupby()` code that calculates daily returns for each ETF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # sorting values to get everything in the right order\n",
    "##> df_etf.sort_values(['symbol', 'date'], inplace=True)\n",
    "##> \n",
    "##> # vectorized return calculation\n",
    "##> df_etf['dly_ret'] = \\\n",
    "##>     df_etf['close'].groupby(df_etf['symbol']).pct_change()\n",
    "##> df_etf.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding `year` and `month` Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate goal is to calculate monthly statistics for each of the ETFs in our data set.\n",
    "\n",
    "As a preliminary step, let's add a month and year column to the `df_etf` by utilizing the `.dt` attribute that `pandas` provides for date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf['year'] = df_etf['date'].dt.year\n",
    "##> df_etf['month'] = df_etf['date'].dt.month\n",
    "##> df_etf[['date', 'year', 'month']].head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick data-integrity check:  There are 10 ETFs in our data set and there are 12 months in a year, so the number of symbol-year-month combinations should be 120.\n",
    "\n",
    "The following code counts the number of rows associated with each symbol-year-month combination and puts that data into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_num_rows = \\\n",
    "##>     df_etf.groupby(['symbol', 'year', 'month']).size().reset_index()\n",
    "##> df_num_rows.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** Confirm that there are the correct number of symbol-year-month combinations in `df_num_rows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've added the `year` and `month` columns we can proceed to calculating our monthly statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Daily Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the most straight-forward calculation: average daily volume, over each month, for all 10 of the ETFs in our data set.  \n",
    "\n",
    "This amounts to:\n",
    "\n",
    "1. grouping by `symbol`, `month`, and `year`\n",
    "\n",
    "1. applying the built-in `np.mean()` function to the `volume` column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_volume = \\\n",
    "##>     df_etf.groupby(['symbol', 'year', 'month'])['volume'].agg([np.mean]).reset_index()\n",
    "##> df_volume.rename(columns={'mean':'avg_volume'}, inplace=True)\n",
    "##> df_volume.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** Calculate the maximum daily volume for each symbol, *over the entire year*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's calculate monthly returns for each of the ETFs in our data set.  This amounts to:\n",
    "\n",
    "1. grouping by `symbol`, `month`, and `year`\n",
    "\n",
    "1. applying an aggregation function to the `daily_returns` column\n",
    "\n",
    "These are the same two steps that we have done in our previous aggregation examples.  However, there is one additional wrinkle that we are going to have to contend with. \n",
    "\n",
    "\n",
    "In the previous section, we used simple built-in aggregation funtions available through `numpy`, such as `np.max` and `np.mean`.  Calculating monthly returns from daily returns is a little more complicated.\n",
    "\n",
    "Thus, we are going to have to first create a custom function for calculating monthly returns from daily returns, and then use this custom function in `.agg()`.\n",
    "\n",
    "The following code defines our monthly returns function in terms of daily returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> def monthly_ret(dly_ret):\n",
    "##>     return np.prod(1 + dly_ret) - 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply `monthly_ret` for all of our ETFs using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_ret = \\\n",
    "##>     df_etf.groupby(['symbol', 'month', 'year'])['dly_ret'].agg([monthly_ret]).reset_index()\n",
    "##> df_ret.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from our calculation that in March of 2020 XLB had a monthly return of -14.6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** Which ETF had the highest single monthly return in all of 2020?  What was the month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a similar process to calculate the monthly volatility for each of the ETFs.\n",
    "\n",
    "We begin by defining a custom function that calculates the monthly volatility from daily returns.  Recall that industry convention is to state these volatilities in annualized terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> def monthly_vol(dly_ret):\n",
    "##>     return np.std(dly_ret) * np.sqrt(252)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our `monthly_vol` function in to perform an aggregating calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_vol = \\\n",
    "##>     df_etf.groupby(['symbol', 'month', 'year'])['dly_ret'].agg([monthly_vol]).reset_index()\n",
    "##> df_vol.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:**  What was the volatility for XLF in December 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Metrics - `inner join`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose that we want to combine our three metrics into one report - meaning that we want them organized into one `DataFrame` in an easy to read fashion.\n",
    "\n",
    "One way to do this is to use the `pandas.merge()` method that we learned in the previous tutorial to join together `df_volume` (average daily volume), `df_ret` (monthly returns), and `df_vol` (monthly volatility). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_joined = \\\n",
    "##>     (\n",
    "##>     df_volume\n",
    "##>         .merge(df_ret, on=['symbol', 'year', 'month'])\n",
    "##>         .merge(df_vol, on=['symbol', 'year', 'month'])\n",
    "##>     )\n",
    "##> df_joined.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Metrics - multiple aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to combine all our statistics into a single `DataFrame` is to supply all of our custom aggregation fuctions as arguments to the `.agg()` function in one shot.  \n",
    "\n",
    "Here is what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # defining aggregations\n",
    "##> agg_funcs = \\\n",
    "##>     {'volume':[np.mean], 'dly_ret':[monthly_ret, monthly_vol]}\n",
    "##> \n",
    "##> # performing all aggregations all three aggregations at once\n",
    "##> df_joined = \\\n",
    "##>     df_etf.groupby(['symbol', 'month', 'year']).agg(agg_funcs).reset_index()\n",
    "##> \n",
    "##> # looking at the data frame\n",
    "##> df_joined.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the input into the `.agg()` method is a `dict` whose elements are pairs that look like: \n",
    "\n",
    "`'column_name':[list_of_aggregating_functions`]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** Modify the code above to add maximum daily volume to the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Reading\n",
    "\n",
    "*PDSH* - 3.7 - Combining Datasets: Merging and Joining\n",
    "\n",
    "*PDSH* - 3.8 - Aggregation and Grouping\n",
    "\n",
    "*Python for Data Analysis (McKinney)* - Chapter 9 (pp 251-274) Data Aggregation and Grouping Operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3",
   "language": "python",
   "name": "py3.7.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
